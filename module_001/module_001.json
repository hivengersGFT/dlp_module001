{"paragraphs":[{"text":"%md                      ## Data Ingestion Team Work MODULE 001 (Maximiliano Gonzalez, Elena Molina, Javier Esteve)\n","user":"anonymous","dateUpdated":"2020-04-16T16:23:05+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":15,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587026459355_-307712106","id":"20200413-132006_1863761526","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:23:05+0000","dateFinished":"2020-04-16T16:23:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:22922"},{"title":"What does the data used contain?","text":"%md \nWe are going to use in this notebook data provided by data.cityofnewyork.us web which is free public data published by New York City agencies and other partners.\nWe pulled two datasets,one of them called Bicycle Counters which contains details of bicycle counters placed around key locations around New York City and another one called Bicycle Counts which gives us the counts that took place in 15 minute windows.\n\nIn the following paragraphs we want to analyze the data from several prespectives:\n1.  Workweek vs weekend Day Averages by Location\n2.  Which is the day of the week with the most bicycle movements over the years.\n3.  Which is the day of the week with the most bicycle movements by year.\n4.1 Calculating by semester, which is the part of the day with more and less bicycle movements during the working days.\n4.2 Calculating by semester, which is the part of the day with more and less bicycle movements during the weekend days.\n\n\nFollow us in this analysis!\n\nLet's start!","user":"anonymous","dateUpdated":"2020-04-16T16:23:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":14,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587026459359_-414790235","id":"20200415-094727_1115565282","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:23:34+0000","dateFinished":"2020-04-16T16:23:34+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22923"},{"title":"Importing needed resources","text":"%sh\napt-get update\n\n#apt-get -y install libgeos-3.5.0\n\n#apt-get -y install libgeos-dev\n\npip install --upgrade pip\n\npip install --upgrade matplotlib\n\npip install --upgrade Pillow\n\n#pip install https://github.com/matplotlib/basemap/archive/master.zip\n\npip install numpy\n\npip install requests\n\nmkdir -p /zeppelin/images\n\n","user":"anonymous","dateUpdated":"2020-04-16T16:37:19+0000","config":{"tableHide":true,"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":294.015,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587026459359_-1128480354","id":"20200408-195156_1386577941","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:04:06+0000","dateFinished":"2020-04-16T16:04:18+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22924"},{"title":"countersTable and dataframe creation","text":"%pyspark\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nimport matplotlib.pyplot as plt\nimport pandas as ps\nfrom pyspark.sql.functions import max as sparkMax\nfrom pyspark.sql.functions import min as sparkMin\n\nsqlContext = SQLContext(sc)\n\ncountersFile = sc.textFile(\"hdfs:///datasets/nyc_data/bicycle_counters.csv\")\n\ncountersFileRDD = countersFile.map(lambda x: x.split(','))\n\nheader = countersFileRDD.first()\n\ncountersRDD2 = countersFileRDD.filter(lambda r: r != header).filter(lambda x: float(x[2]) != 0.0)\n\ndef parseCounters(r):\n    x=Row(id=r[0], name=r[1], latitude=float(r[2]), longitude=float(r[3]))\n    return x\n    \nrowsCountersRDD2 = countersRDD2.map(lambda r: parseCounters(r))\n\ncountersDf = sqlContext.createDataFrame(rowsCountersRDD2)\n\ncountersDf.registerTempTable(\"countersTable\")\n\n#coords = countersDf.select('longitude', 'latitude').collect()\n\n#lon = [x[0] for x in coords]\n#lat = [x[1] for x in coords]\n\n#print (lon)\n#print (lat)\n","user":"anonymous","dateUpdated":"2020-04-16T16:21:40+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=169","http://172.20.0.12:4040/jobs/job?id=170"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459360_176741373","id":"20200408-195705_794263206","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T15:11:47+0000","dateFinished":"2020-04-16T15:11:47+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22925"},{"title":" countsTable  and useful dataframes creation","text":"%pyspark\nfrom pyspark.sql.functions import to_date, to_timestamp, hour, date_format,month\n\ncountsFile = sc.textFile(\"hdfs:///datasets/nyc_data/bicycle_counts.csv\")\n\ncountsFileRDD = countsFile.map(lambda x: x.split(','))\n\nheader = countsFileRDD.first()\n\ncountsRDD2 = countsFileRDD.filter(lambda r: r != header)\n\ndef parseCounts(r):\n    x=Row(id=r[0], date=r[1], counts=r[2])\n    return x\n\nrowsCountsRDD2 = countsRDD2.map(lambda r: parseCounts(r))\n\ncountsDf = sqlContext.createDataFrame(rowsCountsRDD2)\n\ncountsDf = countsDf.withColumn('hour', hour(to_timestamp(countsDf['date'], 'MM/dd/yyyy hh:mm:ss a'))).withColumn('date', to_date(countsDf['date'], 'MM/dd/yyyy'))\n\ncountsDf = countsDf.withColumn(\"day_of_week\", date_format(to_timestamp(countsDf['date'], 'MM/dd/yyyy hh:mm:ss a'), \"u\")).withColumn(\"year\", date_format(to_timestamp(countsDf['date'], 'MM/dd/yyyy hh:mm:ss a'), \"Y\")).withColumn('month', month(to_timestamp(countsDf['date'], 'MM/dd/yyyy hh:mm:ss a')))\n\ncountsDf.registerTempTable(\"countsTable\")\n\ncountsGrouped = sqlContext.sql(\"SELECT id, date, day_of_week, sum(counts) AS grouped_counts FROM countsTable GROUP BY id, date, day_of_week\")\n\ncountsYearGrouped = sqlContext.sql(\"SELECT year, day_of_week, sum(counts) AS grouped_counts FROM countsTable GROUP BY year, day_of_week ORDER BY year, day_of_week\")\n\ncountsGrouped.registerTempTable(\"countsGrouped\")\n\navgDay =  sqlContext.sql(\"SELECT  day_of_week, avg(counts) AS media FROM  countsTable GROUP BY day_of_week order by day_of_week\")\n\ncounts1stTrim =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (1,2,3) and day_of_week <= 5 Group by day_of_week, part_of_day order by day_of_week\")\ncounts2ndTrim =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (4,5,6) and day_of_week <= 5 Group by day_of_week, part_of_day order by day_of_week\")\ncounts3trdTrim =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (7,8,9) and day_of_week <= 5 Group by day_of_week, part_of_day order by day_of_week\")\ncounts4thTrim =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (10,11,12) and day_of_week <= 5 Group by day_of_week, part_of_day order by day_of_week\")\n\ncounts1stTrimWe =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (1,2,3) and day_of_week > 5 Group by day_of_week, part_of_day order by day_of_week\")\ncounts2ndTrimWe =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (4,5,6) and day_of_week > 5  Group by day_of_week, part_of_day order by day_of_week\")\ncounts3trdTrimWe =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (7,8,9) and day_of_week > 5  Group by day_of_week, part_of_day order by day_of_week\")\ncounts4thTrimWe =  sqlContext.sql(\"SELECT day_of_week, sum(counts) grouped_counts, IF(hour  <= 11, 'morning', 'afternoon') as part_of_day FROM countsTable WHERE month in (10,11,12) and day_of_week > 5  Group by day_of_week, part_of_day order by day_of_week\")\ncountsYearGrouped.registerTempTable(\"countsYearGrouped\")\n\n#countsGrouped.show()\n\n\n\n","user":"anonymous","dateUpdated":"2020-04-16T16:29:23+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=237","http://172.20.0.12:4040/jobs/job?id=238"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459360_-445330829","id":"20200409-004520_1952447958","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:29:21+0000","dateFinished":"2020-04-16T16:29:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22926"},{"title":"Preparing data for maps","text":"%pyspark\n\n \n\nworkweek_results = sqlContext.sql(\"SELECT a.id, name, latitude, longitude, avg(grouped_counts) as avg_counted from countsGrouped a, countersTable b WHERE a.id = b.id AND a.day_of_week <= 5 GROUP BY a.id, name, latitude, longitude\").collect()\n\n \n\nweekend_results = sqlContext.sql(\"SELECT a.id, name, latitude, longitude, avg(grouped_counts) as avg_counted from countsGrouped a, countersTable b WHERE a.id = b.id AND a.day_of_week > 5 GROUP BY a.id, name, latitude, longitude\").collect()\n\n \n\n\nworkweek_lat = [row[2] for row in workweek_results]\nworkweek_lon = [row[3] for row in workweek_results]\nworkweek_avg = [int(row[4]) for row in workweek_results]\n\n \n\nweekend_lat = [row[2] for row in weekend_results]\nweekend_lon = [row[3] for row in weekend_results]\nweekend_avg = [int(row[4]) for row in weekend_results]\n\n \n\n#print (workweek_lat)\n#print (workweek_lon)\n#print (workweek_avg)\n\n \n\n#print (weekend_lat)\n#print (weekend_lon)\n#print (weekend_avg)\n\n \n\nllc_lat=countersDf.agg({'latitude': 'min'}).collect()[0][0] -0.01\nllc_lon=countersDf.agg({'longitude': 'min'}).collect()[0][0] -0.01\nurc_lat=countersDf.agg({'latitude': 'max'}).collect()[0][0] + 0.01\nurc_lon=countersDf.agg({'longitude': 'max'}).collect()[0][0] + 0.01","user":"anonymous","dateUpdated":"2020-04-16T16:34:10+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=245","http://172.20.0.12:4040/jobs/job?id=246","http://172.20.0.12:4040/jobs/job?id=247","http://172.20.0.12:4040/jobs/job?id=248","http://172.20.0.12:4040/jobs/job?id=249","http://172.20.0.12:4040/jobs/job?id=250"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459361_-1841974911","id":"20200415-165203_1572270557","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:34:08+0000","dateFinished":"2020-04-16T16:38:17+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22927"},{"title":"1. Workweek vs weekend Day Averages by Location","text":"%pyspark\nimport requests\nimport shutil\n\nBBox = (llc_lon, urc_lon, llc_lat, urc_lat)\n\nbestfit_str = str(llc_lat) + ',' + str(urc_lon) + ',' + str(urc_lat) + ',' + str(llc_lon)\n\nr = requests.get('http://www.mapquestapi.com/staticmap/v4/getmap', stream=True, params={'key':'znNDdg2awMkQEMsyLqIPD8gvO4I0gaxu', 'bestfit' : bestfit_str, 'size' : '996,783', 'imagetype' : 'jpeg'})\nif r.status_code == 200:\n    with open('/zeppelin/images/nyc_map.jpeg', 'wb') as f:\n        r.raw.decode_content = True\n        shutil.copyfileobj(r.raw, f)\n        print(\"s\")\nelse:\n    print (r.text)\n\nnyc_map = plt.imread('/zeppelin/images/nyc_map.jpeg')\n\n\nfig, (ax_map_wd, ax_map_we) = plt.subplots(1, 2, figsize=(25, 10))\n\nax_map_wd.scatter(weekday_lon, weekday_lat, s=weekday_avg, cmap='Blues', alpha=0.5)\n\nax_map_we.scatter(weekend_lon, weekend_lat, s=weekend_avg, cmap='Blues', alpha=0.5)\n\nfor a in [500, 1500, 3000]:\n    ax_map_wd.scatter([], [], c='k', alpha=0.5, s=a, label=str(a) + ' counts')\n    ax_map_we.scatter([], [], c='k', alpha=0.5, s=a, label=str(a) + ' counts')\n\nax_map_wd.legend(scatterpoints=1, labelspacing=3, loc='upper left', handletextpad=1.5, frameon=False)\n\nax_map_we.legend(scatterpoints=1, labelspacing=3, loc='upper left', handletextpad=1.5, frameon=False)\n\nax_map_wd.set_xticks([])\nax_map_wd.set_yticks([])\nax_map_we.set_xticks([])\nax_map_we.set_yticks([])\n\nfig.suptitle(\"Workweek vs Weekend Day Averages\", fontsize=16)\n\nax_map_wd.set_title('Workweek')\n\nax_map_we.set_title('Weekend')\n\nax_map_wd.imshow(nyc_map, extent = BBox,zorder=0, aspect= 'equal')\n\nax_map_we.imshow(nyc_map, extent = BBox,zorder=0, aspect= 'equal')","user":"anonymous","dateUpdated":"2020-04-16T16:39:17+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587026459362_-922457486","id":"20200415-165231_1576727751","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T16:39:17+0000","dateFinished":"2020-04-16T16:39:22+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22928"},{"title":"2. Calculating the average by day of the week.","text":"%pyspark\nimport matplotlib.pyplot as plt\nimport pandas as ps\n\npandas_df = avgDay.toPandas()\npandas_df['day_of_week'] = pandas_df['day_of_week'].astype('int')\npandas_df['media'] = pandas_df['media'].astype('int')\nfig = plt.figure(figsize=(10, 5))\n\nax = plt.gca()\nax.set_ylim(10,20)\npandas_df.plot(kind='bar',x='day_of_week',y='media',color='#9D33FF',ax=ax, label='Media total')\nplt.title('Day of the week Average')\n\nplt.show()\n\n\n\n","user":"anonymous","dateUpdated":"2020-04-16T16:23:55+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"title":true,"results":{"0":{"graph":{"mode":"table","height":87.9297,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=23","http://172.20.0.12:4040/jobs/job?id=24"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459362_-1437413017","id":"20200413-124600_60487529","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T09:05:26+0000","dateFinished":"2020-04-16T09:09:09+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22929"},{"title":"3. Calculating the day with more bicycle activity by year","text":"%pyspark\n\n\ncountsYearGrouped_df = countsYearGrouped.toPandas()\ncountsYearGrouped_df['day_of_week'] = countsYearGrouped_df['day_of_week'].astype('int')\ncountsYearGrouped_df['year'] = countsYearGrouped_df['year'].astype('int')\ncountsYearGrouped_df['grouped_counts'] = countsYearGrouped_df['grouped_counts'].astype('int')\n\nfig, ax = plt.subplots(figsize=(12,5))\nax.set_ylim(5000,4000000)\n\ncountsYearGrouped_df.set_index('day_of_week', inplace=True)\ncountsYearGrouped_df.groupby('year')['grouped_counts'].plot(legend=True)\n\nplt.title('Day of the week counts by year')\nplt.show()\n\n","user":"anonymous","dateUpdated":"2020-04-16T16:24:04+0000","config":{"tableHide":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=21","http://172.20.0.12:4040/jobs/job?id=22"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459363_-732175867","id":"20200414-110931_1216356050","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T09:03:32+0000","dateFinished":"2020-04-16T09:07:19+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22930"},{"title":"4. Activity comparison between workdays and weekend days by part of the day and trimester","text":"%pyspark\n#counts3trdTrim =  sqlContext.sql(\"SELECT  day_of_week, hour, counts FROM countsTable WHERE month in (7,8,9) and day_of_week in (1,2,3,4,5)\")\n#counts4thTrim =  sqlContext.sql(\"SELECT  day_of_week, hour, counts FROM countsTable WHERE month in (10,11,12) and day_of_week in (1,2,3,4,5)\")\n\n\ncounts1stTrim_pd = counts1stTrim.toPandas()\ncounts1stTrim_pd['day_of_week'] = counts1stTrim_pd['day_of_week'].astype('int')\ncounts1stTrim_pd['grouped_counts'] = counts1stTrim_pd['grouped_counts'].astype('int')\ncounts1stTrim_pd['part_of_day'] = counts1stTrim_pd['part_of_day'].astype('str')\n\ncounts2ndTrim_pd = counts2ndTrim.toPandas()\ncounts2ndTrim_pd['day_of_week'] = counts2ndTrim_pd['day_of_week'].astype('int')\ncounts2ndTrim_pd['grouped_counts'] = counts2ndTrim_pd['grouped_counts'].astype('int')\ncounts2ndTrim_pd['part_of_day'] = counts2ndTrim_pd['part_of_day'].astype('str')\n\ncounts3trdTrim_pd = counts3trdTrim.toPandas()\ncounts3trdTrim_pd['day_of_week'] = counts3trdTrim_pd['day_of_week'].astype('int')\ncounts3trdTrim_pd['grouped_counts'] = counts3trdTrim_pd['grouped_counts'].astype('int')\ncounts3trdTrim_pd['part_of_day'] = counts3trdTrim_pd['part_of_day'].astype('str')\n\ncounts4thTrim_pd = counts4thTrim.toPandas()\ncounts4thTrim_pd['day_of_week'] = counts4thTrim_pd['day_of_week'].astype('int')\ncounts4thTrim_pd['grouped_counts'] = counts4thTrim_pd['grouped_counts'].astype('int')\ncounts4thTrim_pd['part_of_day'] = counts4thTrim_pd['part_of_day'].astype('str')\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24,4))\n\nfig.suptitle('Trimestral woorkweek days counters')\nax1.set_title('First Trimester')\nax2.set_title('Second Trimester')\nax3.set_title('Third Trimester')\nax4.set_title('Fourth Trimester')\nax1.set_xticks(range(1,6,1))\nax2.set_xticks(range(1,6,1))\nax3.set_xticks(range(1,6,1))\nax4.set_xticks(range(1,6,1))\n\ncounts1stTrim_pd.set_index('day_of_week', inplace=True)\ncounts1stTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax1)\n\ncounts2ndTrim_pd.set_index('day_of_week', inplace=True)\ncounts2ndTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax2)\n\ncounts3trdTrim_pd.set_index('day_of_week', inplace=True)\ncounts3trdTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax3)\n\ncounts4thTrim_pd.set_index('day_of_week', inplace=True)\ncounts4thTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax4)\n\nplt.show()\n\n\ncounts1stTrim_pd = counts1stTrimWe.toPandas()\ncounts1stTrim_pd['day_of_week'] = counts1stTrim_pd['day_of_week'].astype('int')\ncounts1stTrim_pd['grouped_counts'] = counts1stTrim_pd['grouped_counts'].astype('int')\ncounts1stTrim_pd['part_of_day'] = counts1stTrim_pd['part_of_day'].astype('str')\n\ncounts2ndTrim_pd = counts2ndTrimWe.toPandas()\ncounts2ndTrim_pd['day_of_week'] = counts2ndTrim_pd['day_of_week'].astype('int')\ncounts2ndTrim_pd['grouped_counts'] = counts2ndTrim_pd['grouped_counts'].astype('int')\ncounts2ndTrim_pd['part_of_day'] = counts2ndTrim_pd['part_of_day'].astype('str')\n\ncounts3trdTrim_pd = counts3trdTrimWe.toPandas()\ncounts3trdTrim_pd['day_of_week'] = counts3trdTrim_pd['day_of_week'].astype('int')\ncounts3trdTrim_pd['grouped_counts'] = counts3trdTrim_pd['grouped_counts'].astype('int')\ncounts3trdTrim_pd['part_of_day'] = counts3trdTrim_pd['part_of_day'].astype('str')\n\ncounts4thTrim_pd = counts4thTrimWe.toPandas()\ncounts4thTrim_pd['day_of_week'] = counts4thTrim_pd['day_of_week'].astype('int')\ncounts4thTrim_pd['grouped_counts'] = counts4thTrim_pd['grouped_counts'].astype('int')\ncounts4thTrim_pd['part_of_day'] = counts4thTrim_pd['part_of_day'].astype('str')\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24,4))\n\nfig.suptitle('Trimestral weekend days counters')\nax1.set_title('First Trimester')\nax2.set_title('Second Trimester')\nax3.set_title('Third Trimester')\nax4.set_title('Fourth Trimester')\nax1.set_yticks(range(1000000,3000000,50000))\nax2.set_yticks(range(1000000,3000000,50000))\nax3.set_yticks(range(1000000,3000000,50000))\nax4.set_yticks(range(1000000,3000000,50000))\n\ncounts1stTrim_pd.set_index('day_of_week', inplace=True)\ncounts1stTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax1)\n\ncounts2ndTrim_pd.set_index('day_of_week', inplace=True)\ncounts2ndTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax2)\n\ncounts3trdTrim_pd.set_index('day_of_week', inplace=True)\ncounts3trdTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax3)\n\ncounts4thTrim_pd.set_index('day_of_week', inplace=True)\ncounts4thTrim_pd.groupby('part_of_day')['grouped_counts'].plot(legend=True, ax=ax4)\n\nplt.show()\n\n\n","user":"anonymous","dateUpdated":"2020-04-16T16:24:15+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://172.20.0.12:4040/jobs/job?id=43","http://172.20.0.12:4040/jobs/job?id=44","http://172.20.0.12:4040/jobs/job?id=45","http://172.20.0.12:4040/jobs/job?id=46","http://172.20.0.12:4040/jobs/job?id=47","http://172.20.0.12:4040/jobs/job?id=48","http://172.20.0.12:4040/jobs/job?id=49","http://172.20.0.12:4040/jobs/job?id=50","http://172.20.0.12:4040/jobs/job?id=51","http://172.20.0.12:4040/jobs/job?id=52","http://172.20.0.12:4040/jobs/job?id=53","http://172.20.0.12:4040/jobs/job?id=54","http://172.20.0.12:4040/jobs/job?id=55","http://172.20.0.12:4040/jobs/job?id=56","http://172.20.0.12:4040/jobs/job?id=57","http://172.20.0.12:4040/jobs/job?id=58"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1587026459363_1131213074","id":"20200413-195404_1249650982","dateCreated":"2020-04-16T08:40:59+0000","dateStarted":"2020-04-16T09:24:25+0000","dateFinished":"2020-04-16T09:38:50+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:22931"}],"name":"module_001","id":"2F7B3U3RP","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}